{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing to drop. All good.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import xlrd\n",
    "import os\n",
    "from datetime import datetime, date, timedelta\n",
    "import nltk\n",
    "#from flatten_json import flatten\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "def load_n_explode(file_res=\"API_results_\" + time.strftime(\"%Y%m%d\") + \".csv\"):\n",
    "    # file_res = \"API_results_20210823.csv\"\n",
    "    my_df = pd.read_csv(file_res, error_bad_lines=False)\n",
    "\n",
    "    ## Categories\n",
    "    # my_df[\"Categories\"]\n",
    "    # Willnjust keep 2 levels.\n",
    "    my_df[\"Categories\"] = my_df[\"Categories\"].map(eval, na_action='ignore')\n",
    "    new_df = my_df[\"Categories\"].apply(pd.Series)\n",
    "    my_df[\"Categories\"] = new_df[0].apply(pd.Series).UrlFriendlyName\n",
    "    my_df[\"Sub_Categories\"] = new_df[1].apply(pd.Series).UrlFriendlyName\n",
    "\n",
    "\n",
    "    ###********NEW> LIMIT TO WINES ONLY\n",
    "    #my_df = my_df[my_df['Categories'].isin(['red-wine', 'white-wine'])]\n",
    "    \n",
    "    ## WORK AROUND> NOT SURE WHY. TODO\n",
    "    my_df = my_df[my_df['Stockcode'] != 'ER_2000003422_RX2386']\n",
    "    my_df = my_df[my_df['Stockcode'] != 'ER_1000004375_CALSG16']\n",
    "    my_df = my_df[my_df['AdditionalDetails'] != '[]']\n",
    "\n",
    "\n",
    "    ## Reviews\n",
    "    my_df[\"Reviews\"] = my_df[\"Reviews\"].map(eval, na_action='ignore')\n",
    "    # Try with first 2 reviews\n",
    "    new_df = my_df[\"Reviews\"].apply(pd.Series)\n",
    "    # First\n",
    "    my_df[\"Review1_auth\"] = new_df[0].apply(pd.Series).author.apply(pd.Series).Value\n",
    "    my_df[\"Review1_authorcontent\"] = new_df[0].apply(pd.Series).authorcontent.apply(pd.Series).Value\n",
    "    my_df[\"Review1_points\"] = new_df[0].apply(pd.Series).points.apply(pd.Series).Value\n",
    "    try:\n",
    "        my_df[\"Review1_source\"] = new_df[0].apply(pd.Series).source.apply(pd.Series).Value\n",
    "    except:\n",
    "        my_df[\"Review1_source\"] = \"\"\n",
    "    try:\n",
    "        my_df[\"Review1_text\"] = new_df[0].apply(pd.Series).text.apply(pd.Series).Value\n",
    "    except:\n",
    "        my_df[\"Review1_text\"] = \"\"\n",
    "    try:\n",
    "        my_df[\"Review1_vintage\"] = new_df[0].apply(pd.Series).vintage.apply(pd.Series).Value\n",
    "    except:\n",
    "        my_df[\"Review1_vintage\"] = \"\"\n",
    "        \n",
    "    # Second\n",
    "    try:\n",
    "        my_df[\"Review2_auth\"] = new_df[1].apply(pd.Series).author.apply(pd.Series).Value\n",
    "    except:\n",
    "        my_df[\"Review2_auth\"] = \"\"\n",
    "    try:\n",
    "        my_df[\"Review2_authorcontent\"] = new_df[1].apply(pd.Series).authorcontent.apply(pd.Series).Value\n",
    "    except:\n",
    "        my_df[\"Review2_authorcontent\"] = \"\"\n",
    "    try:\n",
    "        my_df[\"Review2_points\"] = new_df[1].apply(pd.Series).points.apply(pd.Series).Value\n",
    "    except:\n",
    "        my_df[\"Review2_points\"] = \"\"  \n",
    "    try:\n",
    "        my_df[\"Review2_source\"] = new_df[1].apply(pd.Series).source.apply(pd.Series).Value\n",
    "    except:\n",
    "        my_df[\"Review2_source\"] = \"\"\n",
    "    try:\n",
    "        my_df[\"Review2_text\"] = new_df[1].apply(pd.Series).text.apply(pd.Series).Value\n",
    "    except:\n",
    "        my_df[\"Review2_text\"] =\"\"\n",
    "    try:\n",
    "        my_df[\"Review2_vintage\"] = new_df[1].apply(pd.Series).vintage.apply(pd.Series).Value\n",
    "    except:\n",
    "        my_df[\"Review2_vintage\"] = \"\"\n",
    "    \n",
    "    # Drop reviews\n",
    "    my_df = my_df.drop('Reviews', 1)\n",
    "\n",
    "    # Illl make a deep copy for later\n",
    "    full_df = copy.deepcopy(my_df)\n",
    "    # full_df = full_df\n",
    "\n",
    "    # Additional details\n",
    "    my_df[\"AdditionalDetails\"] = my_df[\"AdditionalDetails\"].map(eval, na_action='ignore')\n",
    "    my_df = my_df.drop_duplicates(subset='Stockcode', keep=\"last\")\n",
    "    # Can't use nested lists of JSON objects in pd.json_normalize\n",
    "    my_df = my_df.explode(column=\"AdditionalDetails\").reset_index(drop=True)\n",
    "\n",
    "    # Hacky, but it works... so we wont be touching this stuff!\n",
    "    add_df = pd.DataFrame(pd.json_normalize(my_df[\"AdditionalDetails\"]))\n",
    "    del add_df[\"DisplayName\"]\n",
    "    df = pd.concat([my_df, add_df], axis=1)\n",
    "    df = df.pivot(index='Stockcode', columns='Name', values='Value').reset_index().drop_duplicates(subset=['Stockcode'],\n",
    "                                                                                                   keep=False)\n",
    "\n",
    "      \n",
    "    # Check point, and also a way to get rid of headers\n",
    "    newdf = pd.merge(full_df, df, on='Stockcode')\n",
    "    newdf[\"Mystery\"] = newdf[\"Description\"].str.contains(\"Wraps\")\n",
    "    # This is an old secret seleciton one. Only two, so will drop them\n",
    "    newdf = newdf[~newdf[\"Description\"].str.contains(\"Secret Selection\")]\n",
    "    newdf = newdf[~newdf[\"Stockcode\"].str.contains(\"672366\")]\n",
    "    # newdf = newdf.drop_duplicates(subset=['Stockcode'], keep=False)\n",
    "    \n",
    "    # Dropping a few columns we dont want \n",
    "    newdf = newdf.drop('ProductTags', 1)\n",
    "    newdf = newdf.drop('ProductSashes', 1)\n",
    "    newdf = newdf.drop('UniqueSellingProposition', 1)\n",
    "    newdf = newdf.drop('ImageVariants', 1)\n",
    "    newdf = newdf.drop('AvailablePackTypes', 1)\n",
    "    try:\n",
    "        newdf = newdf.drop('GroupedDetails.image', 1)\n",
    "    except:\n",
    "        print(\"Nothing to drop. All good.\")\n",
    "    try:\n",
    "        newdf = newdf.drop('GroupedDetails.video', 1)\n",
    "    except:\n",
    "        print(\"Nothing to drop. All good.\")\n",
    "    newdf = newdf.drop('GroupedDetails.workflow', 1)\n",
    "    newdf = newdf.drop('categoryleafnodeid', 1)\n",
    "    # Now to drop the additional details, we no need no more.\n",
    "    newdf = newdf.drop('AdditionalDetails', 1)\n",
    "    \n",
    "    return newdf\n",
    "\n",
    "\n",
    "\n",
    "os.chdir('/home/stu/code/dans/files')\n",
    "#input_file = \"API_results_20210903.csv\"\n",
    "input_file = \"API_results_\" + time.strftime(\"%Y%m%d\") + \".csv\"\n",
    "wide = load_n_explode(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table raw_dans_raw_main created/updated successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user = 'root'\n",
    "passw = 'MYSQLl0g1n!'\n",
    "host =  '127.0.0.1'\n",
    "port = 3306\n",
    "database = 'dans_dev'\n",
    "\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pymysql\n",
    "import pandas as pd\n",
    "\n",
    "#sqlEngine       = create_engine('mysql+pymysql://root:@127.0.0.1/test', pool_recycle=3600)\n",
    "sqlEngine = create_engine('mysql+mysqlconnector://' + user + ':' + passw + '@' + host + ':' + str(port) + '/' + database , echo=False)\n",
    "dbConnection    = sqlEngine.connect()\n",
    "tableName = \"raw_dans_raw_main\"\n",
    "\n",
    "try:\n",
    "    frame           = wide.to_sql(tableName, dbConnection, if_exists='append', method='multi', chunksize = 1000 );\n",
    "except ValueError as vx:\n",
    "    print(vx)\n",
    "except Exception as ex:   \n",
    "    print(ex)\n",
    "else:\n",
    "    print(\"Table %s created/updated successfully.\"%tableName);   \n",
    "finally:\n",
    "    dbConnection.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
