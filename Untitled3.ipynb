{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import xlrd\n",
    "import os\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "#from flatten_json import flatten\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "import time\n",
    "import copy\n",
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "\n",
    "def load_n_explode(file_res=\"API_results_\" + time.strftime(\"%Y%m%d\") + \".csv\"):\n",
    "    # file_res = \"API_results_20210823.csv\"\n",
    "    my_df = pd.read_csv(file_res)\n",
    "\n",
    "    ## Categories\n",
    "    # my_df[\"Categories\"]\n",
    "    # Willnjust keep 2 levels.\n",
    "    my_df[\"Categories\"] = my_df[\"Categories\"].map(eval, na_action='ignore')\n",
    "    new_df = my_df[\"Categories\"].apply(pd.Series)\n",
    "    my_df[\"Categories\"] = new_df[0].apply(pd.Series).UrlFriendlyName\n",
    "    my_df[\"Sub_Categories\"] = new_df[1].apply(pd.Series).UrlFriendlyName\n",
    "\n",
    "    ## Reviews\n",
    "    my_df[\"Reviews\"] = my_df[\"Reviews\"].map(eval, na_action='ignore')\n",
    "    # Try with first 2 reviews\n",
    "    new_df = my_df[\"Reviews\"].apply(pd.Series)\n",
    "    # First\n",
    "    my_df[\"Review1_auth\"] = new_df[0].apply(pd.Series).author.apply(pd.Series).Value\n",
    "    my_df[\"Review1_authorcontent\"] = new_df[0].apply(pd.Series).authorcontent.apply(pd.Series).Value\n",
    "    my_df[\"Review1_points\"] = new_df[0].apply(pd.Series).points.apply(pd.Series).Value\n",
    "    my_df[\"Review1_source\"] = new_df[0].apply(pd.Series).source.apply(pd.Series).Value\n",
    "    my_df[\"Review1_text\"] = new_df[0].apply(pd.Series).text.apply(pd.Series).Value\n",
    "    my_df[\"Review1_vintage\"] = new_df[0].apply(pd.Series).vintage.apply(pd.Series).Value\n",
    "    # Second\n",
    "    my_df[\"Review2_auth\"] = new_df[1].apply(pd.Series).author.apply(pd.Series).Value\n",
    "    my_df[\"Review2_authorcontent\"] = new_df[1].apply(pd.Series).authorcontent.apply(pd.Series).Value\n",
    "    my_df[\"Review2_points\"] = new_df[1].apply(pd.Series).points.apply(pd.Series).Value\n",
    "    my_df[\"Review2_source\"] = new_df[1].apply(pd.Series).source.apply(pd.Series).Value\n",
    "    my_df[\"Review2_text\"] = new_df[1].apply(pd.Series).text.apply(pd.Series).Value\n",
    "    my_df[\"Review2_vintage\"] = new_df[1].apply(pd.Series).vintage.apply(pd.Series).Value\n",
    "\n",
    "    # Illl make a deep copy for later\n",
    "    full_df = copy.deepcopy(my_df)\n",
    "    # full_df = full_df\n",
    "\n",
    "    # Additional details\n",
    "    my_df[\"AdditionalDetails\"] = my_df[\"AdditionalDetails\"].map(eval, na_action='ignore')\n",
    "    # Can't use nested lists of JSON objects in pd.json_normalize\n",
    "    my_df = my_df.explode(column=\"AdditionalDetails\").reset_index(drop=True)\n",
    "\n",
    "    # Hacky, but it works... so we wont be touching this stuff!\n",
    "    add_df = pd.DataFrame(pd.json_normalize(my_df[\"AdditionalDetails\"]))\n",
    "    del add_df[\"DisplayName\"]\n",
    "    df = pd.concat([my_df, add_df], axis=1)\n",
    "    df = df.pivot(index='Stockcode', columns='Name', values='Value').reset_index().drop_duplicates(subset=['Stockcode'],\n",
    "                                                                                                   keep=False)\n",
    "\n",
    "    # Check point, and also a way to get rid of headers\n",
    "    newdf = pd.merge(full_df, df, on='Stockcode')\n",
    "    newdf[\"Mystery\"] = newdf[\"Description\"].str.contains(\"Wraps\")\n",
    "    # This is an old secret seleciton one. Only two, so will drop them\n",
    "    newdf = newdf[~newdf[\"Description\"].str.contains(\"Secret Selection\")]\n",
    "    newdf = newdf[~newdf[\"Stockcode\"].str.contains(\"672366\")]\n",
    "    # newdf = newdf.drop_duplicates(subset=['Stockcode'], keep=False)\n",
    "    return newdf\n",
    "\n",
    "\n",
    "\n",
    "def giveaway(df):\n",
    "    gives = df.copy(deep=True)\n",
    "    gives = gives[['Stockcode','Description','webproductname','Prices.singleprice.Value','Prices.promoprice.Value','Prices.promoprice.BeforePromotion','Prices.promoprice.AfterPromotion','IsForDelivery']]\n",
    "    gives = gives[gives.webproductname.notnull()]\n",
    "    gives = gives[gives.webproductname.notna()]\n",
    "    gives = gives[gives[\"Description\"].str.lower().str.contains(\"Wraps\")]\n",
    "    gives = gives[~gives[\"webproductname\"].str.lower().str.contains(\"Wraps\")]\n",
    "    gives = gives[gives[\"IsForDelivery\"]]\n",
    "    gives[\"METHOD\"] = \"giveaway\"\n",
    "    return gives\n",
    "\n",
    "\n",
    "\n",
    "def ohe(df):\n",
    "    #### ONE HOT ENCODED\n",
    "    ##### First I split into numeric and nominal. OHE the nominal\n",
    "    known=df\n",
    "    exclude_col = known.select_dtypes(include=np.number).columns.tolist() + [\"Stockcode\"]\n",
    "    my_df_num = known[exclude_col]\n",
    "    my_df_cat = known.drop(exclude_col, axis=1)\n",
    "    # my_df_cat.to_csv(\"FIN.csv\")\n",
    "    my_df_cat_ohe = pd.get_dummies(my_df_cat)\n",
    "    my_df_ohe = pd.concat([my_df_num, my_df_cat_ohe], axis=1)\n",
    "    my_df_ohe = my_df_ohe.fillna(0)\n",
    "    my_df_ohe = my_df_ohe.replace(np.nan, 0)\n",
    "\n",
    "    # Drop duplicates #TODO check whats better to keep\n",
    "    my_df_ohe = my_df_ohe.loc[:, ~my_df_ohe.columns.duplicated()]\n",
    "    # Clean up names\n",
    "    my_df_ohe.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in\n",
    "                         my_df_ohe.columns.values]\n",
    "\n",
    "    return my_df_ohe\n",
    "\n",
    "def get_knn(df, thresh):\n",
    "\n",
    "    myst  = df[df[\"Mystery\"]]\n",
    "    known  = df[~df[\"Mystery\"]]\n",
    "\n",
    "    myst_nn = myst.drop(\"Stockcode\", axis=1)\n",
    "    myst_nn = myst_nn.drop(\"Mystery\", axis=1)\n",
    "    known_nn = known.drop(\"Stockcode\", axis=1)\n",
    "    known_nn = known_nn.drop(\"Mystery\", axis=1)\n",
    "\n",
    "    # Create the k-NN model using k=5\n",
    "    nn_abs = NearestNeighbors(n_neighbors=1, algorithm='auto')\n",
    "\n",
    "    # Fit it\n",
    "    nn_abs.fit(known_nn)\n",
    "\n",
    "    results_wine = []\n",
    "\n",
    "    for index in range(len(myst.index)):\n",
    "        distance, matches = nn_abs.kneighbors(myst_nn.iloc[[index]], 1, return_distance=True)\n",
    "        results_wine.append(\n",
    "            {\n",
    "                #'Mystery': \"https://www.danmurphys.com.au/product/\" + str(myst['Stockcode'].iloc[[index][0]]),\n",
    "                'Stockcode_x': str(myst['Stockcode'].iloc[[index][0]]),\n",
    "                #'Matched': \"https://www.danmurphys.com.au/product/\" + str(known[\"Stockcode\"].iloc[matches[0][0]]),\n",
    "                'Stockcode_y': str(known[\"Stockcode\"].iloc[matches[0][0]]),\n",
    "                'Distance': str(distance[0][0])\n",
    "\n",
    "            }\n",
    "        )\n",
    "\n",
    "    matched = pd.DataFrame(results_wine)\n",
    "    matched[\"MatchLevel\"] = np.where(matched['Distance'].astype(float) < float(thresh), \"Good\", \"Poor\")\n",
    "    #matched = matched.sort_values(['MatchLevel', 'Savings'], ascending=[True, False])\n",
    "    matched[\"METHOD\"] = \"KNN\"\n",
    "    return matched\n",
    "\n",
    "\n",
    "def lazy_desc(df, keep1):\n",
    "    #df = wide\n",
    "    #keep1 = keep_nlp\n",
    "    kept = df[keep1]\n",
    "\n",
    "    kept.reset_index(drop=True, inplace=True)\n",
    "    myst = kept[kept[\"Mystery\"]]\n",
    "    known = kept[~kept[\"Mystery\"]]\n",
    "\n",
    "    # Description match\n",
    "    desc_match = pd.merge(myst[myst['RichDescription'].notna()], known, on=['RichDescription'], how='inner')\n",
    "    desc_match[\"METHOD\"] = \"desc_match\"\n",
    "    # Web match\n",
    "    webdesc_match = pd.merge(myst[myst['webdescriptionshort'].notna()], known, on=['webdescriptionshort'], how='inner')\n",
    "    webdesc_match[\"METHOD\"] = \"webdesc_match\"\n",
    "    # Review match (TODO more than 2 deep)\n",
    "    rev_match = pd.merge(myst[myst['Review1_text'].notna()], known, on=['Review1_text'], how='inner')\n",
    "    rev_match[\"METHOD\"] = \"rev_match\"\n",
    "    #rev_match = rev_match[~[rev_match['Review1_text'] == [...]]]\n",
    "    rev1_match = pd.merge(myst[myst['Review2_text'].notna()], known, on=['Review2_text'], how='inner')\n",
    "    rev1_match[\"METHOD\"] = \"rev2_match\" # I know. Using normal index for people...\n",
    "    #rev_match = rev_match[~[rev_match.['Review1_text'].str.contains(\"[...]\")]]\n",
    "    text_match = desc_match.append(webdesc_match).append(rev_match).append(rev1_match).reset_index()\n",
    "    text_match = text_match[['Stockcode_x', 'Stockcode_y',\"METHOD\"]]\n",
    "    text_match[\"MatchLevel\"] = \"Good\"\n",
    "    return text_match\n",
    "\n",
    "\n",
    "## Try Fuzzywuzzy\n",
    "\n",
    "def fuzzy_merge(df_1, df_2, key1, key2, threshold=90, limit=1):\n",
    "    \"\"\"\n",
    "    :param df_1: the left table to join\n",
    "    :param df_2: the right table to join\n",
    "    :param key1: key column of the left table\n",
    "    :param key2: key column of the right table\n",
    "    :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n",
    "    :param limit: the amount of matches that will get returned, these are sorted high to low\n",
    "    :return: dataframe with boths keys and matches\n",
    "    \"\"\"\n",
    "    s = df_2[key2].tolist()\n",
    "\n",
    "    m = df_1[key1].apply(lambda x: process.extract(x, s, limit=limit))\n",
    "    df_1['matches'] = m\n",
    "\n",
    "    m2 = df_1['matches'].apply(lambda x: ', '.join([i[0] for i in x if i[1] >= threshold]))\n",
    "    df_1['matches'] = m2\n",
    "\n",
    "    return df_1\n",
    "\n",
    "#fuzzy_merge(myst, known, 'webdescriptionshort', 'webdescriptionshort', threshold=80)\n",
    "\n",
    "\n",
    "def make_clickable(val):\n",
    "    # target _blank to open new window\n",
    "    val1 = 'https://www.danmurphys.com.au/product/' + str(val)\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val1, val)\n",
    "\n",
    "\n",
    "## CONFIG\n",
    "\n",
    "keep_nlptdf =['Stockcode',\n",
    " 'webdescriptionshort',\n",
    "'RichDescription']\n",
    "\n",
    "keep_ohe =['Categories',\n",
    " 'Mystery',\n",
    " 'Stockcode',\n",
    " 'PackageSize',\n",
    " 'Prices.inanysixprice.Message',\n",
    " 'Prices.inanysixprice.Value',\n",
    " 'Sub_Categories',\n",
    " 'Review1_auth',\n",
    " 'Review1_points',\n",
    " 'Review1_source',\n",
    " 'awardwinner',\n",
    " 'glutenfree',\n",
    " 'preservativefree',\n",
    " 'varietal',\n",
    " 'webalcoholpercentage',\n",
    " 'webbottleclosure',\n",
    " 'webcountryoforigin',\n",
    " 'webfoodmatch',\n",
    " 'webisorganic',\n",
    " 'webisvegan',\n",
    " 'webliquorsize',\n",
    " 'webmaincategory',\n",
    " 'webregionoforigin',\n",
    " 'webstateoforigin',\n",
    " 'webtotalreviewcount',\n",
    " 'webwinebody',\n",
    " 'webwinestyle',\n",
    " 'IsForDelivery']\n",
    "\n",
    "\n",
    "keep_nlp =['Categories',\n",
    " 'Stockcode',\n",
    " 'Mystery',\n",
    " 'PackageSize',\n",
    " 'RichDescription',\n",
    " 'Review1_text',\n",
    " 'Review2_text',\n",
    " 'Prices.inanysixprice.Message',\n",
    " 'Prices.inanysixprice.Value',\n",
    " 'Sub_Categories',\n",
    " 'Review1_auth',\n",
    " 'Review1_points',\n",
    " 'Review1_source',\n",
    " 'awardwinner',\n",
    " 'glutenfree',\n",
    " 'preservativefree',\n",
    " 'varietal',\n",
    " 'webalcoholpercentage',\n",
    " 'webbottleclosure',\n",
    " 'webcountryoforigin',\n",
    " 'webdescriptionshort',\n",
    " 'webfoodmatch',\n",
    " 'webisorganic',\n",
    " 'webisvegan',\n",
    " 'webliquorsize',\n",
    " 'webmaincategory',\n",
    " 'webregionoforigin',\n",
    " 'webstateoforigin',\n",
    " 'webtotalreviewcount',\n",
    " 'webwinebody',\n",
    " 'webwinestyle',\n",
    " 'IsForDelivery']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9f79244a36ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"API_results_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwide\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_n_explode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-4f6cf6fd22e7>\u001b[0m in \u001b[0;36mload_n_explode\u001b[0;34m(file_res)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m# Hacky, but it works... so we wont be touching this stuff!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0madd_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"AdditionalDetails\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0madd_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DisplayName\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_normalize.py\u001b[0m in \u001b[0;36m_json_normalize\u001b[0;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;31m# TODO: handle record value which are lists, at least error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0;31m#       reasonably\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_to_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/json/_normalize.py\u001b[0m in \u001b[0;36mnested_to_record\u001b[0;34m(ds, prefix, sep, level, max_level)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mnew_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;31m# each key gets renamed with prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "#### RUNNING HERE\n",
    "\n",
    "input_file = \"API_results_\" + time.strftime(\"%Y%m%d\") + \".csv\"\n",
    "wide = load_n_explode(input_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide.to_csv(\"wide_today.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wide' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a12b66b4ba04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Get the easy matches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgiveaway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwide\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlazy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy_desc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwide\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_nlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#now the OHE and KNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wide' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "## Get the easy matches\n",
    "gives = giveaway(wide)\n",
    "lazy = lazy_desc(wide,keep_nlp)\n",
    "\n",
    "#now the OHE and KNN\n",
    "ohe_file = ohe(wide[keep_ohe])\n",
    "knns = get_knn(ohe_file,1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and save it\n",
    "matches = knns.append(lazy)\n",
    "\n",
    "# Check if its in stock\n",
    "avail =wide[[\"Stockcode\",'varietal','Prices.singleprice.Value','Prices.promoprice.Value','Prices.promoprice.BeforePromotion','Prices.promoprice.AfterPromotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.merge(matches, avail, left_on='Stockcode_x', right_on=\"Stockcode\")\n",
    "matches[\"Savings\"] = matches['Prices.promoprice.BeforePromotion'] - matches['Prices.promoprice.AfterPromotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matches = pd.merge(matches, wide[[\"Description\",\"Stockcode\"]], left_on='Stockcode_y', right_on=\"Stockcode\")\n",
    "#matches.columns = ['Stockcode_x', 'Stockcode_y', 'Distance', 'MatchLevel', 'Stockcode_x', 'varietal', 'Prices.singleprice.Value', 'Prices.promoprice.Value', 'Prices.promoprice.BeforePromotion', 'Prices.promoprice.AfterPromotion', 'Savings', 'Description', 'Stockcode_Z']\n",
    "matches = matches[matches.MatchLevel.str.contains(\"Good\")]\n",
    "\n",
    "matches = matches[['Stockcode_x', 'Stockcode_y', 'Description', 'varietal', 'Prices.promoprice.BeforePromotion', 'Prices.promoprice.AfterPromotion', 'Savings']]\n",
    "matches = matches.sort_values(['Savings'], ascending=[False])\n",
    "\n",
    "matches = matches.loc[:,~matches.columns.duplicated()]\n",
    "\n",
    "matches = matches.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches[\"Stockcode_x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18181818181818182"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches[\"Stockcode_x\"])/len(wide[wide[\"Mystery\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "style is not supported for non-unique indices.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-940b182235d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Stockcode_x'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmake_clickable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Stockcode_y'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmake_clickable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Savings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'#5fba7d'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Savings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'#5fba7d'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mstyle\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStyler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mStyler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     _shared_docs[\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/formats/style.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, precision, table_styles, uuid, caption, table_attributes, cell_ids, na_rep)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"style is not supported for non-unique indices.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: style is not supported for non-unique indices."
     ]
    }
   ],
   "source": [
    "\n",
    "csv_file = \"Match_results\" + time.strftime(\"%Y%m%d\") + \".csv\"\n",
    "matches.to_csv(csv_file)\n",
    "\n",
    "from datetime import date, timedelta  \n",
    "yesterday = date.today() - timedelta(days=1) \n",
    "today = date.today()   \n",
    "file_yes = \"Match_results\" + yesterday.strftime(\"%Y%m%d\") + \".csv\" \n",
    "file_tod = \"Match_results\" + today.strftime(\"%Y%m%d\") + \".csv\"\n",
    "df1 = pd.read_csv(file_yes).iloc[:, 1:]\n",
    "df2 = pd.read_csv(file_tod).iloc[:, 1:]\n",
    "df_diff = pd.concat([df1,df2]).drop_duplicates(keep=False)\n",
    "\n",
    "matches = df_diff\n",
    "\n",
    "\n",
    "final = matches.style.format({'Stockcode_x': make_clickable, 'Stockcode_y': make_clickable, }) \\\n",
    "    .bar(subset=['Savings'], align='mid', color=['#5fba7d']) \\\n",
    "    .bar(subset=['Savings'], align='mid', color=['#5fba7d']) \\\n",
    "    .hide_index()\n",
    "\n",
    "\n",
    "#writing HTML Content\n",
    "heading = '<h1> Matched wines</h1>'\n",
    "subheading = '<h3> Results sub header </h3>'\n",
    "# Using .now() from datetime library to add Time stamp\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "header = '<div class=\"top\">' + heading + subheading +'</div>'\n",
    "footer = '<div class=\"bottom\"> <h3> This Report has been Generated on'+ current_time +'</h3> </div>'\n",
    "content = final\n",
    "# Concating everything to a single string\n",
    "\n",
    "html = header + content.render() + footer\n",
    "html_file = \"Match_new.html\"\n",
    "with open(html_file,'w+') as file:\n",
    "    file.write(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import xlrd\n",
    "import os\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import datetime, date, timedelta\n",
    "import nltk\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "#from flatten_json import flatten\n",
    "import time\n",
    "\n",
    "import glob\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/home/stu/code/dans' # use your path\n",
    "all_files = glob.glob(path + \"/sitemap.xml\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-91c76f5b5917>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-91c76f5b5917>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    wines = wines[wines.'<?xml version=\"1.0\" encoding=\"utf-8\"?>'.str.contains('https://www.danmurphys.com.au/product',case=False, na=False)]\u001b[0m\n\u001b[0m                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# And now to cleanup results, and remove duplicates\n",
    "\n",
    "wines = frame\n",
    "# And now I'll filter out the product linkes\n",
    "wines = wines[wines.'<?xml version=\"1.0\" encoding=\"utf-8\"?>'.str.contains('https://www.danmurphys.com.au/product',case=False, na=False)]\n",
    "\n",
    "wines = wines.replace(r'.*https://www.danmurphys.com.au/product/DM_', r'', regex=True)\n",
    "wines = wines.replace(r'/.*$', r'', regex=True)\n",
    "wines = wines.drop_duplicates()\n",
    "wines = wines.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
