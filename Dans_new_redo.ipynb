{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import xlrd\n",
    "import os\n",
    "from datetime import datetime, date, timedelta\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "#from flatten_json import flatten\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "import time\n",
    "import copy\n",
    "from fuzzywuzzy import process, fuzz\n",
    "\n",
    "\n",
    "def load_n_explode(file_res=\"API_results_\" + time.strftime(\"%Y%m%d\") + \".csv\"):\n",
    "    # file_res = \"API_results_20210823.csv\"\n",
    "    my_df = pd.read_csv(file_res, error_bad_lines=False)\n",
    "\n",
    "    ## Categories\n",
    "    # my_df[\"Categories\"]\n",
    "    # Willnjust keep 2 levels.\n",
    "    my_df[\"Categories\"] = my_df[\"Categories\"].map(eval, na_action='ignore')\n",
    "    new_df = my_df[\"Categories\"].apply(pd.Series)\n",
    "    my_df[\"Categories\"] = new_df[0].apply(pd.Series).UrlFriendlyName\n",
    "    my_df[\"Sub_Categories\"] = new_df[1].apply(pd.Series).UrlFriendlyName\n",
    "\n",
    "\n",
    "    ###********NEW> LIMIT TO WINES ONLY\n",
    "    my_df = my_df[my_df['Categories'].isin(['red-wine', 'white-wine'])]\n",
    "\n",
    "    ## WORK AROUND> NOT SURE WHY. TODO\n",
    "    my_df = my_df[my_df['Stockcode'] != 'ER_2000003422_RX2386']\n",
    "    my_df = my_df[my_df['Stockcode'] != 'ER_1000004375_CALSG16']\n",
    "\n",
    "    \n",
    "\n",
    "    ## Reviews\n",
    "    my_df[\"Reviews\"] = my_df[\"Reviews\"].map(eval, na_action='ignore')\n",
    "    # Try with first 2 reviews\n",
    "    new_df = my_df[\"Reviews\"].apply(pd.Series)\n",
    "    # First\n",
    "    my_df[\"Review1_auth\"] = new_df[0].apply(pd.Series).author.apply(pd.Series).Value\n",
    "    my_df[\"Review1_authorcontent\"] = new_df[0].apply(pd.Series).authorcontent.apply(pd.Series).Value\n",
    "    my_df[\"Review1_points\"] = new_df[0].apply(pd.Series).points.apply(pd.Series).Value\n",
    "    my_df[\"Review1_source\"] = new_df[0].apply(pd.Series).source.apply(pd.Series).Value\n",
    "    my_df[\"Review1_text\"] = new_df[0].apply(pd.Series).text.apply(pd.Series).Value\n",
    "    my_df[\"Review1_vintage\"] = new_df[0].apply(pd.Series).vintage.apply(pd.Series).Value\n",
    "    # Second\n",
    "    my_df[\"Review2_auth\"] = new_df[1].apply(pd.Series).author.apply(pd.Series).Value\n",
    "    my_df[\"Review2_authorcontent\"] = new_df[1].apply(pd.Series).authorcontent.apply(pd.Series).Value\n",
    "    my_df[\"Review2_points\"] = new_df[1].apply(pd.Series).points.apply(pd.Series).Value\n",
    "    my_df[\"Review2_source\"] = new_df[1].apply(pd.Series).source.apply(pd.Series).Value\n",
    "    my_df[\"Review2_text\"] = new_df[1].apply(pd.Series).text.apply(pd.Series).Value\n",
    "    my_df[\"Review2_vintage\"] = new_df[1].apply(pd.Series).vintage.apply(pd.Series).Value\n",
    "\n",
    "    # Illl make a deep copy for later\n",
    "    full_df = copy.deepcopy(my_df)\n",
    "    # full_df = full_df\n",
    "\n",
    "    # Additional details\n",
    "    my_df[\"AdditionalDetails\"] = my_df[\"AdditionalDetails\"].map(eval, na_action='ignore')\n",
    "    # Can't use nested lists of JSON objects in pd.json_normalize\n",
    "    my_df = my_df.explode(column=\"AdditionalDetails\").reset_index(drop=True)\n",
    "\n",
    "    # Hacky, but it works... so we wont be touching this stuff!\n",
    "    add_df = pd.DataFrame(pd.json_normalize(my_df[\"AdditionalDetails\"]))\n",
    "    del add_df[\"DisplayName\"]\n",
    "    df = pd.concat([my_df, add_df], axis=1)\n",
    "    df = df.pivot(index='Stockcode', columns='Name', values='Value').reset_index().drop_duplicates(subset=['Stockcode'],\n",
    "                                                                                                   keep=False)\n",
    "\n",
    "    # Check point, and also a way to get rid of headers\n",
    "    newdf = pd.merge(full_df, df, on='Stockcode')\n",
    "    newdf[\"Mystery\"] = newdf[\"Description\"].str.lower().str.contains(\"wraps\")\n",
    "    # This is an old secret seleciton one. Only two, so will drop them\n",
    "    newdf = newdf[~newdf[\"Description\"].str.contains(\"Secret Selection\")]\n",
    "    newdf = newdf[~newdf[\"Stockcode\"].str.contains(\"672366\")]\n",
    "    # newdf = newdf.drop_duplicates(subset=['Stockcode'], keep=False)\n",
    "    return newdf\n",
    "\n",
    "\n",
    "\n",
    "def giveaway(df):\n",
    "    gives = df.copy(deep=True)\n",
    "    gives = gives[['Stockcode','Description','webproductname','Prices.singleprice.Value','Prices.promoprice.Value','Prices.promoprice.BeforePromotion','Prices.promoprice.AfterPromotion','IsForDelivery']]\n",
    "    gives = gives[gives.webproductname.notnull()]\n",
    "    gives = gives[gives.webproductname.notna()]\n",
    "    gives = gives[gives[\"Description\"].str.lower().str.contains(\"wraps\")]\n",
    "    gives = gives[~gives[\"webproductname\"].str.lower().str.contains(\"wraps\")]\n",
    "    gives = gives[gives[\"IsForDelivery\"]]\n",
    "    gives[\"METHOD\"] = \"giveaway\"\n",
    "    return gives\n",
    "\n",
    "\n",
    "\n",
    "def ohe(df):\n",
    "    #### ONE HOT ENCODED\n",
    "    ##### First I split into numeric and nominal. OHE the nominal\n",
    "    known=df\n",
    "    exclude_col = known.select_dtypes(include=np.number).columns.tolist() + [\"Stockcode\"]\n",
    "    my_df_num = known[exclude_col]\n",
    "    my_df_cat = known.drop(exclude_col, axis=1)\n",
    "    # my_df_cat.to_csv(\"FIN.csv\")\n",
    "    my_df_cat_ohe = pd.get_dummies(my_df_cat)\n",
    "    my_df_ohe = pd.concat([my_df_num, my_df_cat_ohe], axis=1)\n",
    "    my_df_ohe = my_df_ohe.fillna(0)\n",
    "    my_df_ohe = my_df_ohe.replace(np.nan, 0)\n",
    "\n",
    "    # Drop duplicates #TODO check whats better to keep\n",
    "    my_df_ohe = my_df_ohe.loc[:, ~my_df_ohe.columns.duplicated()]\n",
    "    # Clean up names\n",
    "    my_df_ohe.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in\n",
    "                         my_df_ohe.columns.values]\n",
    "\n",
    "    return my_df_ohe\n",
    "\n",
    "def get_knn(df, thresh):\n",
    "\n",
    "    myst  = df[df[\"Mystery\"]]\n",
    "    known  = df[~df[\"Mystery\"]]\n",
    "\n",
    "    myst_nn = myst.drop(\"Stockcode\", axis=1)\n",
    "    myst_nn = myst_nn.drop(\"Mystery\", axis=1)\n",
    "    known_nn = known.drop(\"Stockcode\", axis=1)\n",
    "    known_nn = known_nn.drop(\"Mystery\", axis=1)\n",
    "\n",
    "    # Create the k-NN model using k=5\n",
    "    nn_abs = NearestNeighbors(n_neighbors=1, algorithm='auto')\n",
    "\n",
    "    # Fit it\n",
    "    nn_abs.fit(known_nn)\n",
    "\n",
    "    results_wine = []\n",
    "\n",
    "    for index in range(len(myst.index)):\n",
    "        distance, matches = nn_abs.kneighbors(myst_nn.iloc[[index]], 1, return_distance=True)\n",
    "        results_wine.append(\n",
    "            {\n",
    "                #'Mystery': \"https://www.danmurphys.com.au/product/\" + str(myst['Stockcode'].iloc[[index][0]]),\n",
    "                'Stockcode_x': str(myst['Stockcode'].iloc[[index][0]]),\n",
    "                #'Matched': \"https://www.danmurphys.com.au/product/\" + str(known[\"Stockcode\"].iloc[matches[0][0]]),\n",
    "                'Stockcode_y': str(known[\"Stockcode\"].iloc[matches[0][0]]),\n",
    "                'Distance': str(distance[0][0])\n",
    "\n",
    "            }\n",
    "        )\n",
    "\n",
    "    matched = pd.DataFrame(results_wine)\n",
    "    matched[\"MatchLevel\"] = np.where(matched['Distance'].astype(float) < float(thresh), \"Good\", \"Poor\")\n",
    "    #matched = matched.sort_values(['MatchLevel', 'Savings'], ascending=[True, False])\n",
    "    matched[\"METHOD\"] = \"KNN\"\n",
    "    return matched\n",
    "\n",
    "\n",
    "def lazy_desc(df, keep1):\n",
    "    #df = wide\n",
    "    #keep1 = keep_nlp\n",
    "    kept = df[keep1]\n",
    "\n",
    "    kept.reset_index(drop=True, inplace=True)\n",
    "    myst = kept[kept[\"Mystery\"]]\n",
    "    known = kept[~kept[\"Mystery\"]]\n",
    "    known = known[known[\"Review1_text\"] != \"[...]\"]\n",
    "    known = known[known[\"Review2_text\"] != \"[...]\"]\n",
    "    \n",
    "    known = known[known[\"Review1_text\"] != \"[..]\"]\n",
    "    known = known[known[\"Review2_text\"] != \"[..]\"]\n",
    "    \n",
    "    known = known[known[\"Review1_text\"] != \"[....]\"]\n",
    "    known = known[known[\"Review2_text\"] != \"[....]\"]\n",
    " \n",
    "    \n",
    "    # Description match\n",
    "    desc_match = pd.merge(myst[myst['RichDescription'].notna()], known, on=['RichDescription'], how='inner')\n",
    "    desc_match[\"METHOD\"] = \"desc_match\"\n",
    "    # Web match\n",
    "    webdesc_match = pd.merge(myst[myst['webdescriptionshort'].notna()], known, on=['webdescriptionshort'], how='inner')\n",
    "    webdesc_match[\"METHOD\"] = \"webdesc_match\"\n",
    "    # Review match (TODO more than 2 deep)\n",
    "    rev_match = pd.merge(myst[myst['Review1_text'].notna()], known, on=['Review1_text'], how='inner')\n",
    "    rev_match[\"METHOD\"] = \"rev_match\"\n",
    "    #rev_match = rev_match[~[rev_match['Review1_text'] == [...]]]\n",
    "    rev1_match = pd.merge(myst[myst['Review2_text'].notna()], known, on=['Review2_text'], how='inner')\n",
    "    rev1_match[\"METHOD\"] = \"rev2_match\" # I know. Using normal index for people...\n",
    "    #rev_match = rev_match[~[rev_match.['Review1_text'].str.contains(\"[...]\")]]\n",
    "    text_match = desc_match.append(webdesc_match).append(rev_match).append(rev1_match).reset_index()\n",
    "    text_match = text_match[['Stockcode_x', 'Stockcode_y',\"METHOD\"]]\n",
    "    text_match[\"MatchLevel\"] = \"Good\"\n",
    "    return text_match\n",
    "\n",
    "\n",
    "## Try Fuzzywuzzy\n",
    "\n",
    "def fuzzy_merge(df_1, df_2, key1, key2, threshold=90, limit=1):\n",
    "    \"\"\"\n",
    "    :param df_1: the left table to join\n",
    "    :param df_2: the right table to join\n",
    "    :param key1: key column of the left table\n",
    "    :param key2: key column of the right table\n",
    "    :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n",
    "    :param limit: the amount of matches that will get returned, these are sorted high to low\n",
    "    :return: dataframe with boths keys and matches\n",
    "    \"\"\"\n",
    "    s = df_2[key2].tolist()\n",
    "\n",
    "    m = df_1[key1].apply(lambda x: process.extract(x, s, limit=limit))\n",
    "    df_1['matches'] = m\n",
    "\n",
    "    m2 = df_1['matches'].apply(lambda x: ', '.join([i[0] for i in x if i[1] >= threshold]))\n",
    "    df_1['matches'] = m2\n",
    "\n",
    "    return df_1\n",
    "\n",
    "#fuzzy_merge(myst, known, 'webdescriptionshort', 'webdescriptionshort', threshold=80)\n",
    "\n",
    "\n",
    "def make_clickable(val):\n",
    "    # target _blank to open new window\n",
    "    val1 = 'https://www.danmurphys.com.au/product/' + str(val)\n",
    "    return '<a target=\"_blank\" href=\"{}\">{}</a>'.format(val1, val)\n",
    "\n",
    "\n",
    "## CONFIG\n",
    "\n",
    "keep_nlptdf =['Stockcode',\n",
    " 'webdescriptionshort',\n",
    "'RichDescription']\n",
    "\n",
    "keep_ohe =['Categories',\n",
    " 'Mystery',\n",
    " 'Stockcode',\n",
    " 'PackageSize',\n",
    " 'Prices.inanysixprice.Message',\n",
    " 'Prices.inanysixprice.Value',\n",
    " 'Sub_Categories',\n",
    " 'Review1_auth',\n",
    " 'Review1_points',\n",
    " 'Review1_source',\n",
    " 'awardwinner',\n",
    " 'glutenfree',\n",
    " 'preservativefree',\n",
    " 'varietal',\n",
    " 'webalcoholpercentage',\n",
    " 'webbottleclosure',\n",
    " 'webcountryoforigin',\n",
    " 'webfoodmatch',\n",
    " 'webisorganic',\n",
    " 'webisvegan',\n",
    " 'webliquorsize',\n",
    " 'webmaincategory',\n",
    " 'webregionoforigin',\n",
    " 'webstateoforigin',\n",
    " 'webtotalreviewcount',\n",
    " 'webwinebody',\n",
    " 'webwinestyle',\n",
    " 'IsForDelivery']\n",
    "\n",
    "\n",
    "keep_nlp =['Categories',\n",
    " 'Stockcode',\n",
    " 'Mystery',\n",
    " 'PackageSize',\n",
    " 'RichDescription',\n",
    " 'Review1_text',\n",
    " 'Review2_text',\n",
    " 'Prices.inanysixprice.Message',\n",
    " 'Prices.inanysixprice.Value',\n",
    " 'Sub_Categories',\n",
    " 'Review1_auth',\n",
    " 'Review1_points',\n",
    " 'Review1_source',\n",
    " 'awardwinner',\n",
    " 'glutenfree',\n",
    " 'preservativefree',\n",
    " 'varietal',\n",
    " 'webalcoholpercentage',\n",
    " 'webbottleclosure',\n",
    " 'webcountryoforigin',\n",
    " 'webdescriptionshort',\n",
    " 'webfoodmatch',\n",
    " 'webisorganic',\n",
    " 'webisvegan',\n",
    " 'webliquorsize',\n",
    " 'webmaincategory',\n",
    " 'webregionoforigin',\n",
    " 'webstateoforigin',\n",
    " 'webtotalreviewcount',\n",
    " 'webwinebody',\n",
    " 'webwinestyle',\n",
    " 'IsForDelivery']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stu/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3254: DtypeWarning: Columns (101,106) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    }
   ],
   "source": [
    "#### RUNNING HERE\n",
    "\n",
    "#input_file = \"API_results_\" + time.strftime(\"%Y%m%d\") + \".csv\"\n",
    "input_file = \"API_results_20210906.csv\"\n",
    "wide = load_n_explode(input_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = wide[wide['Stockcode'] != 'ER_2000003439_MTRESSHZ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the easy matches\n",
    "gives = giveaway(wide)\n",
    "lazy = lazy_desc(wide,keep_nlp)\n",
    "\n",
    "#now the OHE and KNN\n",
    "ohe_file = ohe(wide[keep_ohe])\n",
    "knns = get_knn(ohe_file,1.5)\n",
    "\n",
    "# Cleanup and save it\n",
    "#matches = knns.append(lazy)\n",
    "matches = lazy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if its in stock\n",
    "avail = wide[wide[\"IsForDelivery\"]][[\"Stockcode\",'varietal','Prices.singleprice.Value','Prices.promoprice.Value','Prices.promoprice.BeforePromotion','Prices.promoprice.AfterPromotion']]\n",
    "\n",
    "matches = pd.merge(matches, avail, left_on='Stockcode_x', right_on=\"Stockcode\")\n",
    "matches[\"Savings\"] = matches['Prices.promoprice.BeforePromotion'] - matches['Prices.promoprice.AfterPromotion']\n",
    "matches = pd.merge(matches, wide[[\"Description\",\"Stockcode\"]], left_on='Stockcode_y', right_on=\"Stockcode\")\n",
    "#matches.columns = ['Stockcode_x', 'Stockcode_y', 'Distance', 'MatchLevel', 'Stockcode_x', 'varietal', 'Prices.singleprice.Value', 'Prices.promoprice.Value', 'Prices.promoprice.BeforePromotion', 'Prices.promoprice.AfterPromotion', 'Savings', 'Description', 'Stockcode_Z']\n",
    "matches = matches[matches.MatchLevel.str.contains(\"Good\")]\n",
    "\n",
    "matches = matches[['Stockcode_x', 'Stockcode_y', 'Description', 'varietal', 'Prices.promoprice.BeforePromotion', 'Prices.promoprice.AfterPromotion', 'Savings']]\n",
    "matches = matches.sort_values(['Savings'], ascending=[False])\n",
    "\n",
    "matches = matches.loc[:,~matches.columns.duplicated()]\n",
    "\n",
    "matches = matches.drop_duplicates()\n",
    "\n",
    "csv_file = \"Match_results\" + time.strftime(\"%Y%m%d\") + \".csv\"\n",
    "matches.to_csv(csv_file)\n",
    "\n",
    "#from datetime import date, timedelta  \n",
    "#yesterday = date.today() - timedelta(days=1) \n",
    "#today = date.today()   \n",
    "#file_yes = \"Match_results\" + yesterday.strftime(\"%Y%m%d\") + \".csv\" \n",
    "#file_tod = \"Match_results\" + today.strftime(\"%Y%m%d\") + \".csv\"\n",
    "#df1 = pd.read_csv(file_yes).iloc[:, 1:]\n",
    "#df2 = pd.read_csv(file_tod).iloc[:, 1:]\n",
    "#df_diff = pd.concat([df1,df2]).drop_duplicates(keep=False)\n",
    "\n",
    "#matches = df_diff\n",
    "\n",
    "\n",
    "final = matches.style.format({'Stockcode_x': make_clickable, 'Stockcode_y': make_clickable, }) \\\n",
    "    .bar(subset=['Savings'], align='mid', color=['#5fba7d']) \\\n",
    "    .bar(subset=['Savings'], align='mid', color=['#5fba7d']) \\\n",
    "    .hide_index()\n",
    "\n",
    "\n",
    "#writing HTML Content\n",
    "heading = '<h1> Matched wines</h1>'\n",
    "subheading = '<h3> Results sub header </h3>'\n",
    "# Using .now() from datetime library to add Time stamp\n",
    "now = datetime.now()\n",
    "current_time = now.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "header = '<div class=\"top\">' + heading + subheading +'</div>'\n",
    "footer = '<div class=\"bottom\"> <h3> This Report has been Generated on'+ current_time +'</h3> </div>'\n",
    "content = final\n",
    "# Concating everything to a single string\n",
    "\n",
    "html = header + content.render() + footer\n",
    "html_file = \"Match_new.html\"\n",
    "with open(html_file,'w+') as file:\n",
    "    file.write(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cork = wide[wide[\"Mystery\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cork = cork[cork[\"webbottleclosure\"] == \"Cork\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cork = pd.merge(cork, avail, left_on='Stockcode', right_on=\"Stockcode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cork[['Stockcode','Description', 'varietal', 'Prices.promoprice.BeforePromotion', 'Prices.promoprice.AfterPromotion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## match on \n",
    "variety\n",
    "Cost per bottle\n",
    "closure\n",
    "standard drinks\n",
    "alcohol %\n",
    "region\n",
    "size\n",
    "\n",
    "Once matched, then if the count is greater than 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
